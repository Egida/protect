DESIGN

-------------

Client ping near relay requirements

    1. Can have more relays to ping than can fit in one response packet

    2. Pass down lowest cost from each relay so we can self direct the optimization to give up early on relays that don't make sense

    3. The common case should be fast. If we have a lot of good relays with no packet loss and low jitter, we can stop after just one second.

    4. If we don't have any good relays that we can communicate across we can retry until we reach the connect timeout. The worst can be slower, but taking its time to connect.

    5. The client doesn't connect until it finds the route through the relays, so we don't have the first 10 seconds of the match being unaccelerated.

I can mock up and test everything above with the sha256 hash, so the relay ping/pongs are compatible with the current fleet, so I can have a LOT of relays that I can ping.

-------------

Another thing to consider is that we need the ip2location results as input to the query for near relays

We currently don't have anything setup to do the ip2location

We should probably solve this first?

I can't do the ip2location lookup inside the XDP program, but what I could do is have ip2location results cached inside a bpf map, and then only drop down to the userspace socket (AF_XDP?) if the ip address is not in the cache.

This should all be done inside the client backend, because we'll have multiple instances of these across the world (close to players) and we don't want them relying on a sub-service that's somewhere else.

So the client backend will need some loop where it periodically refreshes and downloads the ip2location database from some central location (or more likely, from multiple ones...)

-------------

We have a similar pattern when performaning autodetect for servers.

We won't be able to do this autodetect (whois) inside the XDP program, but we can have a cache in an XDP map, and then in the bpf program, just return the cached value, otherwise forward packet to userspace socket.

-------------

We should also be implementing some rate limiting per-user hash at the XDP program level

Otherwise, once somebody has a backend token they could go wide with that and try to attack the server with lots of requests

n requests per-minute enforced by XDP dropping anything that comes in faster than that

How to implement this efficiently with BPF maps?

-------------

We should also check the IP address and port that the client backend token was created for.

We will only ever respond if the request is coming from the same IP:port.

This way a backend token cannot be taken wide.

-------------


------------------------------------------------------------------------------------------


Client operation (accelerated):

    1. Client generates a connect token (this would be given to it by the game's backend...). This connect token basically says "timestamp: the client user hash X from buyer Y wants to connect to server id Z", signed by some secret private key corresponding to the buyer, for which we know the public key. It should be super simple and stupid to generate (server id is just the hash of the public address + port of the server) so it doesn't require the customer to hit our backend from their backend to generate it.

    2. The client passes the connect token to the client backend, which checks it and returns some "client state" (encrypted with backend private key) to be used on any other calls, plus enough information for the client to start pinging near relays.

    3. The client pings near relays for 10 seconds. Possibly with an early out at 5 seconds if there is low jitter and packet loss (common case --> fast connect).

    4. The client passes the relay ping results up to the client backend with the "client state".

    5. The client backend may evolve the client state and return it back down, along with "OK, here are two routes to use to server" *or* "Try pinging relays again, here are the relays to ping", *or* "Fail". We do this because if decisions are made primarily on the backend then it can be must easier to tune and fix. We need to keep the SDK as stupid as possible.

    6. The client punches the two routes through relays, and once they are both confirmed considers itself connected and the game can start sending UDP packets.

    7. Every 10 seconds the client renews the two routes. If one route is not confirmed, the system will keep trying, potentially passing down a different route for the route that is not confirmed, or even generating two different routes randomly.

    8. After n retries (probably 30 seconds...) the connect will fail.

    9. Every 5 minutes the client will be told to do near relay pings again, again, the client can be told to retry this n times.

    10. The timeouts for anything should be generous (for example 30 seconds, not 10)

    11. The goal is to be relaxed and allow clients to weather temporary periods of broken connectivity without being disconnected.

    12. The client should also have a clean disconnect to the server, sent across relays whenever the route changes (to close down the old route...), and to the server in the end (sent across both routes together...), so the server knows that the client has disconnected clean, or 

    13. We might need to implement some fast reconnect logic when the same user hash tries to connect to the server again, while there is a timing out entry for that user hash (30 seconds+ timeout...)


Client operation (non-accelerated):

    The client is given an ip address + port as a string.

    The client just opens a socket and sends packets (with prefix header etc...) to this address, with a packet type that says, I'm direct.

    The server responds, with the same prefix header and direct packet type.

    This way we can bypass the backend entirely and just connect direct during development, local play etc.

    During production we can lock servers down so they don't support direct connections, only connections through a connect token.


DETAIL

    ----------

	Client token is fixed size:

        [timestamp]             8 bytes
        [user_hash]             8 bytes
        [session_id]            8 bytes
        [server_id]             8 bytes
        [buyer_id]              8 bytes
        [client_backends]       (ipv6 address+port * n bytes, where n = number of backends, say 32...)
        [ping_rate]             4 bytes (default 10 pings per-second, per-backend)
        [ping_retries]          4 bytes (default 10)
        (signature)

    The client needs access to some data in it, so it should be signed, but not encrypted.

    We rely on the game backend transmitting it to the client securely over HTTPS or however they choose to do it.

    ----------

    The client reads the client token and extracts server backends and tries to init and ping each one in ||.

    Each frame it stops and checks if it has a response with more than half the pings responded to (in the last second...)

    If it finds one or more, then it randomly selects from the set with the most pings responded (tie break), and that's the backend it chooses to use.

    If it doesn't find any client backend to talk to, it tries again n times (as per-connect token data)

    This should connect to the best client backend for wherever the client is located (by latency), and by packet loss.

    ----------

    I'd like to implement the client backend as an XDP program

    The client backend will check if the timestamp is not expired, and that the buyer is a known buyer id, and then checks the signature for the connect token, and ignores if it has expired.

    The init should then respond with a "client state" which is a smaller encrypted block of state that is passed back and forth between the client and the backend.

    ----------

    At minimum the client state should be:

        (nonce)
        [timestamp]
        [user_hash]
        [session_id]
        [server_id]
        [client_relay_ids]
        [client_relay_rtts]
        [client_relay_jitter]
        [client_relay_pl]
        (hmac)

    ----------

    The client backend also supports a ping/pong, with the client state being passed in (to make the ping request packet larger than the pong response).

    To reply with a pong the "client state" must not have expired (20 seconds?)

    ----------

    In order to do everything above, because we are fixed size (for both XDP and to avoid length extension attacks) we just need the SHA256 hash on client and backend.

    ----------



NOTES

------

All comms between the client and the backend should be UDP

This allows us to do smart stuff in the future, like multiple backends for init, and we talk to the lowest latency one, or backends inside mainland china vs. outside, or multi-region, or multi-cloud.

------

Should the multiple backends be passed down to the client inside the connect token? Probably...

We should probably do like a few seconds handshake init, to pick the client backend we can talk to. Assume that some players won't be able to talk to all backends.

I'd like as close to zero configuration stuff living in the client, period.

------

We probably want there to be some token or acknowledgement that comes from the server that *confirms* the client is connected to it, so we can enforce that we only enable this client to keep hitting our backend as long as it actually is able to talk to the server.

------

We also want to protect against sharing of connect tokens, so we can do the same thing we did in space game with the whole server disconnects if the user hash is no longer assigned to it (newest client connecting takes over per-user hash, ie. only one connection per-user hash allowed)

------

One question is whether we can fit two routes into the one UDP response packet, or if we need to have two response packets for the one request, or maybe two requests, one per-route.

------

The route state should be pretty minimal, compared with the previous network next logic.

------

Zero copy packet send and receive design on client and server.

------

Players cannot connect without a next route, so we can no longer use [0,254] for near relays. We should increase to [0,1022] and use 1023 for not routable.

------

Can we have more than 16 near relays?

------

Both the server and the relays should have an explicit close session so we can detect clean vs. dirty disconnects

------

No golang. I want the backends written in XDP and C for the most efficient scaling of components, especially the client and server backends.

------
