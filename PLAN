
---------------------------------------

Client creates socket

Client socket is singlethreaded. No background thread. Just a UDP socket.

Client communicates directly to the backend over UDP.

Client is pumped at game framerate (60HZ variable, give or take...).

Client inits with backend and gets a backend token (as per-space game).

Client has to refresh this backend token at some rate, like every 60 seconds.

Client requests a ping token from the backend (passing in backend token) and the server id (hash of ip address and port) it wants to connect to. The response is an encoding of the relays to ping, with time limited ping tokens.

The pings are quantized to game framerate. We aren't trying to find small minimal latency improvements here, just pick the best relays within the set. Any improvement less than one frame is ignored.

Client pings the relays its given for a certain amount of time (and is able to request a new set of ping tokens and repeat pings until it is happy with the a result)

Once the the client has the ping results, they can be accessed as a base64 string.

Passing up this base64 string to the backend, the client can now be given routes.

Aim to have multiple routes active at the same time. Refresh them once per-second, and be persistent, if we can't communicate over the routes retry until we can. Don't give up!

There is no fallback to direct. Failing to stay connected to the backend = the client disconnects. Failing to renew routes should be tolerated, and it's OK to retry again and again until eventually giving up.

The client will have clean shutdown of routes and any backend comms, so we can detect hard disconnects vs. clean disconnects.

---------------------------------------

Server never exposes IP address to clients. It rolls a random 64bit server id at startup and registers under this id with the server backend. This is how it is identified across the system (eg. to connect to)

Server starts up and inits with backend.

Server is probably hitting a completely different backend instance, eg. "server_backend" and there is a "client_backend" etc.

The server does server init and update as it does currently, although if it fails, it waits and tries again. 

The server cannot just fallback to direct only mode anymore. If the server is not connected to the backend, clients cannot connect to it.

So it must retry pretty aggressively, and even if it can't talk to the backend for a short while, it should keep retrying. Timeouts should be long (30 seconds).

Servers should have a clean shutdown and let the backend know when they have cleanly shutdown, so we can track clean vs. dirty shutdown.

The server also finds near relays, which starts with relays in the same datacenter, but also, other near relays by lat/long.

This gives us multiple ways to "get in" to the server, which we need since we never expose the server IP address.

The last hop is quantified and the cost of it is added to the route (vs. being limited to 1-2ms of latency.... in can be more now)

The server is running at game framerate with basic sockets and there are no crazy things going on.

The server makes up reserved IP addresses, because it really never knows the actual client address anymore.

All the ping response and stuff is done off-main thread via XDP programs (like a relay on the same machine as server).

This way we can avoid any silly background thread stuff per-server instance.

---------------------------------------

Server backend listens for server inits and updates and tracks them in redis.

---------------------------------------

Relay backend listens for relay updates

---------------------------------------

Client backend handles clients initing and updating, plus routes

---------------------------------------

Optimizer runs in the background and does the whole optimize thing with redundancy

---------------------------------------

Yes, there is still terraform, and a REST API via "api" service

and there is postgres behind running the configuration -> database.bin

---------------------------------------

Since this system requires 100% reliability, it should be multi-region for all components (client/server/relay backend, and optimizer...)

---------------------------------------

If we want to relax from protect, we can optionally pass down the server IP address as an additional route

---------------------------------------

Relays need to be upgraded to listen to handle session close request/response

This allows us to detect clean vs. session disconnects on a per-relay basis

---------------------------------------
